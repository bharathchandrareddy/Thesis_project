{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis of model performance on in-domain-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"### add_the_model_path_ of_second_solution ##\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models\n",
    "from torchvision.models import ResNet34_Weights\n",
    "from legacy.zoo.models import *\n",
    "from datasets.base_dataset import LabeledDataset\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import skimage\n",
    "import skimage.io\n",
    "import torch\n",
    "from albumentations import Compose, PadIfNeeded\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import models\n",
    "from dataset.xview_dataset import XviewSingleDataset\n",
    "from tools.config import load_config\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the arguments\n",
    "config = 'configs/r50.json'  # Path to configuration file\n",
    "data_path = '/home/selim/datasets/xview/train/'  # Path to test images\n",
    "gpu = '0'  # GPU to use\n",
    "output_dir = '../predictions/xview/r50_dice'\n",
    "mask_dir = '../predictions/masks'\n",
    "model_path = 'weights2/spacenet_resnext_unet_resnext50_0_best_dice'   # add the 2nd model weights path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "conf = load_config(config)\n",
    "\n",
    "# Initialize the model\n",
    "model = models.__dict__[conf['network']](seg_classes=5, backbone_arch=conf['encoder'])\n",
    "model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "# Load model weights\n",
    "print(f\"=> Loading checkpoint '{model_path}'\")\n",
    "checkpoint = torch.load(model_path, map_location=\"cpu\")\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function to process datafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from typing import Dict, Union, Any, List\n",
    "\n",
    "def normalize_image(img: torch.Tensor) -> torch.Tensor:\n",
    "    img = img.to(torch.float32)\n",
    "    img /= 127\n",
    "    img -= 1\n",
    "    return img\n",
    "\n",
    "\n",
    "def process_images(images_folder: str, masks_folder: str) -> List[Dict[str, Union[torch.Tensor, Any]]]:\n",
    "    results = []\n",
    "\n",
    "    for image_filename in os.listdir(images_folder):\n",
    "        if '_pre_' not in image_filename:\n",
    "            continue\n",
    "\n",
    "        # Construct full paths for pre-disaster and post-disaster images\n",
    "        img_pre_path = os.path.join(images_folder, image_filename)\n",
    "        img_post_path = img_pre_path.replace('_pre_', '_post_')\n",
    "\n",
    "        # Construct full paths for pre-disaster and post-disaster masks\n",
    "        msk_pre_path = os.path.join(masks_folder, image_filename)\n",
    "        msk_post_path = msk_pre_path.replace('_pre_disaster', '_post_disaster')\n",
    "\n",
    "        # Print paths for debugging\n",
    "        # print(f\"Processing image: {image_filename}\")\n",
    "        # print(f\"img_pre_path: {img_pre_path}\")\n",
    "        # print(f\"img_post_path: {img_post_path}\")\n",
    "        # print(f\"msk_pre_path: {msk_pre_path}\")\n",
    "        # print(f\"msk_post_path: {msk_post_path}\")\n",
    "\n",
    "        # Read images\n",
    "        img_pre = cv2.imread(img_pre_path, cv2.IMREAD_COLOR)\n",
    "        img_post = cv2.imread(img_post_path, cv2.IMREAD_COLOR)\n",
    "\n",
    "        # Read masks\n",
    "        msk_pre = cv2.imread(msk_pre_path, cv2.IMREAD_UNCHANGED)\n",
    "        msk_post = cv2.imread(msk_post_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "        if img_pre is None or img_post is None or msk_pre is None or msk_post is None:\n",
    "            print(f\"Skipping file: {image_filename}, missing images or masks.\")\n",
    "            continue\n",
    "\n",
    "        # Ensure masks are integer type\n",
    "        msk_pre = msk_pre.astype(np.uint8)\n",
    "        msk_post = msk_post.astype(np.uint8)\n",
    "\n",
    "        # Print unique values in masks\n",
    "        # print(f\"Unique values in msk_pre: {np.unique(msk_pre)}\")\n",
    "        # print(f\"Unique values in msk_post: {np.unique(msk_post)}\")\n",
    "\n",
    "        # Create separate masks for different disaster levels\n",
    "        msk0 = (msk_pre > 0).astype(np.uint8)[..., np.newaxis]\n",
    "        msk1 = (msk_post == 1).astype(np.uint8)[..., np.newaxis]\n",
    "        msk2 = (msk_post == 2).astype(np.uint8)[..., np.newaxis]\n",
    "        msk3 = (msk_post == 3).astype(np.uint8)[..., np.newaxis]\n",
    "        msk4 = (msk_post == 4).astype(np.uint8)[..., np.newaxis]\n",
    "\n",
    "        # Combine masks\n",
    "        msk = np.concatenate([msk0, msk1, msk2, msk3, msk4], axis=2)\n",
    "\n",
    "        # Concatenate pre and post-disaster images\n",
    "        img = np.concatenate([img_pre, img_post], axis=2)\n",
    "\n",
    "        # Convert to tensors\n",
    "        img_tensor = torch.from_numpy(img.transpose((2, 0, 1))).float()\n",
    "        msk_tensor = torch.from_numpy(msk.transpose((2, 0, 1))).float()\n",
    "\n",
    "        # Normalize the image tensor\n",
    "        img_tensor = normalize_image(img_tensor)\n",
    "\n",
    "        # Add the processed image and mask tensors to the result list\n",
    "        results.append({'img': img_tensor, 'msk': msk_tensor,'fn':image_filename})\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms (empty in this script)\n",
    "transforms = Compose([])\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = XviewSingleDataset(data_path=data_path, transforms=transforms, mode=\"val\", \n",
    "                             normalize=conf['input'].get('normalize', None))\n",
    "data_loader = DataLoader(dataset, batch_size=1, num_workers=8, shuffle=False, pin_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform inference\n",
    "# with torch.no_grad():\n",
    "#     for sample in tqdm(data_loader):\n",
    "#         ids = sample['img_name']\n",
    "#         image = sample['image'].numpy()[0]\n",
    "        \n",
    "#         # Test-time augmentation (TTA)\n",
    "#         images = np.array([\n",
    "#             image,\n",
    "#             image[:, ::-1, :],  # Horizontal flip\n",
    "#             image[:, :, ::-1],  # Vertical flip\n",
    "#             image[:, ::-1, ::-1],  # Both flips\n",
    "#         ])\n",
    "#         images = torch.from_numpy(images).cuda().float()\n",
    "        \n",
    "#         # Model prediction\n",
    "#         logits = model(images)\n",
    "#         preds = torch.sigmoid(logits).cpu().numpy()\n",
    "        \n",
    "#         # Post-process predictions\n",
    "#         prediction_masks = []\n",
    "#         for i in range(4):\n",
    "#             pred = preds[i]\n",
    "#             if i == 1:\n",
    "#                 pred = preds[i].copy()[:, ::-1, :]\n",
    "#             if i == 2:\n",
    "#                 pred = preds[i].copy()[:, :, ::-1]\n",
    "#             if i == 3:\n",
    "#                 pred = preds[i].copy()[:, ::-1, ::-1]\n",
    "#             prediction_masks.append(pred)\n",
    "#         preds = np.average(prediction_masks, axis=0)\n",
    "\n",
    "#         # Normalize and save predictions\n",
    "#         preds = (np.moveaxis(preds, 0, -1) * 255).astype(np.uint8)\n",
    "#         skimage.io.imsave(os.path.join(output_dir, ids[0] + \"_localization.png\"), preds[:, :, 4])\n",
    "#         skimage.io.imsave(os.path.join(output_dir, ids[0] + \"_damage.png\"), preds[:, :, :-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "import torchmetrics.classification\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_model(model, test_loader, threshold=0.38, device='cuda'):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test dataloader and display confusion matrices.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained model to evaluate.\n",
    "        test_loader (torch.utils.data.DataLoader): Dataloader for the test dataset.\n",
    "        threshold (float): Threshold for the location mask.\n",
    "        device (str): Device to use for evaluation ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of F1 scores per channel and displays confusion matrices.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    model.to(device)\n",
    "\n",
    "    num_classes = 2  # Assuming binary classification\n",
    "    test_metrics = [torchmetrics.F1Score(num_classes=num_classes, task='binary', average='none').to(device) for _ in range(5)]  # Assuming 5 channels\n",
    "    #confusion_matrices = [torchmetrics.ConfusionMatrix(num_classes=num_classes,task='multiclass').to(device) for _ in range(5)]  # Confusion matrices for 5 channels\n",
    "    # Reset metrics\n",
    "    for metric in test_metrics :\n",
    "        metric.reset()\n",
    "    results = {}\n",
    "\n",
    "    with torch.no_grad():  # No need for gradient computation during evaluation\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            x, y = batch[\"img\"].float().to(device), batch[\"msk\"].float().to(device)\n",
    "            y_hat = model(x).float()  # Forward pass\n",
    "            \n",
    "\n",
    "            # Apply sigmoid activation\n",
    "            y_sigm = torch.sigmoid(y_hat)\n",
    "\n",
    "            # Location prediction mask\n",
    "            # to distuinguish between building presence or absense\n",
    "            loc_pred = y_sigm[:, 0, ...]\n",
    "            loc_msk = (loc_pred > threshold)  # Binary mask based on threshold\n",
    "\n",
    "            # Damage mask: 5-class prediction per pixel\n",
    "            dmg_msk = y_sigm[:, 1:, ...].argmax(axis=1) + 1\n",
    "            dmg_msk = dmg_msk * loc_msk  # Combine location and damage masks\n",
    "\n",
    "            # One-hot encode the damage mask\n",
    "            hot_dmg_msk = torch.zeros_like(y_hat, dtype=y_hat.dtype)\n",
    "            for i in range(5):\n",
    "                hot_dmg_msk[:, i, ...] = (dmg_msk == i)\n",
    "            hot_dmg_msk[:, 0, ...] = loc_msk  # Update location channel\n",
    "            \n",
    "            # Update metrics for each channel\n",
    "            for i in range(y_hat.shape[1]):\n",
    "                test_metrics[i].update(hot_dmg_msk[:, i, ...], y[:, i, ...])\n",
    "                preds = hot_dmg_msk[:, i, ...].flatten()\n",
    "                targets = y[:, i, ...].flatten()\n",
    "                \n",
    "                # Print unique values\n",
    "                # print(f\"Batch {batch_idx}, Channel {i}\")\n",
    "                # print(f\"Unique values in predictions: {torch.unique(preds)}\")\n",
    "                # print(f\"Unique values in targets: {torch.unique(targets)}\")\n",
    "                #confusion_matrices[i].update(hot_dmg_msk[:, i, ...], y[:, i, ...])\n",
    "            # if batch_idx == 2:  # Limit to first 3 batches\n",
    "            #     break\n",
    "\n",
    "    #Compute the final F1 scores and display confusion matrices for each channel\n",
    "    for i in range(len(test_metrics)):\n",
    "        results[f\"test_channel_{i}_F1\"] = test_metrics[i].compute().item()\n",
    "\n",
    "    #     # Plot confusion matrix\n",
    "    #     cm = confusion_matrices[i].compute().cpu().numpy()\n",
    "    #     plt.figure(figsize=(6, 6))\n",
    "    #     sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, \n",
    "    #                 xticklabels=[f'Class_{j}' for j in range(num_classes)], \n",
    "    #                 yticklabels=[f'Class_{j}' for j in range(num_classes)])\n",
    "    #     plt.title(f\"Confusion Matrix for Channel {i}\")\n",
    "    #     plt.xlabel(\"Predicted\")\n",
    "    #     plt.ylabel(\"Actual\")\n",
    "    #     plt.show()\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "model_dir = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\prediction_masks\\\\no_aug\"\n",
    "\n",
    "# Define a color mapping for the damage mask (5 classes: 0-4)\n",
    "damage_color_map = {\n",
    "    0: [0, 0, 0],       # Class 0 (background) - black\n",
    "    1: [0, 255, 0],     # Class 1 (minor damage) - green\n",
    "    2: [255, 255, 0],   # Class 2 (moderate damage) - yellow\n",
    "    3: [255, 165, 0],   # Class 3 (major damage) - orange\n",
    "    4: [255, 0, 0],     # Class 4 (destroyed) - red\n",
    "}\n",
    "\n",
    "# Define a color mapping for the localization mask (binary: 0 and 1)\n",
    "localization_color_map = {\n",
    "    0: [0, 0, 0],       # Not localized - black\n",
    "    1: [0, 255, 0],     # Localized - green\n",
    "}\n",
    "\n",
    "def mask_to_color(mask, color_map):\n",
    "    \"\"\"Convert a single-channel mask to a 3-channel color image.\"\"\"\n",
    "    height, width = mask.shape\n",
    "    color_img = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    \n",
    "    for class_id, color in color_map.items():\n",
    "        color_img[mask == class_id] = color\n",
    "        \n",
    "    return color_img\n",
    "\n",
    "def pred_one_image(pred, threshold=0.38):\n",
    "    \"\"\"\n",
    "    Processes the model's prediction for a single image, creating localization and damage masks.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): The model's raw output for a single image.\n",
    "        threshold (float): Threshold for binary localization mask.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: (loc_msk, dmg_msk)\n",
    "            loc_msk: Localization mask (binary).\n",
    "            dmg_msk: Damage mask (multi-class).\n",
    "    \"\"\"\n",
    "    y_sigm = torch.sigmoid(pred)  # Apply sigmoid activation to the prediction\n",
    "    y_pred = y_sigm.cpu().numpy().transpose(1, 2, 0)  # Convert to numpy and move channel to last position\n",
    "    \n",
    "    loc_pred = y_pred[..., 0]  # Get the localization prediction (first channel)\n",
    "    loc_msk = (loc_pred > threshold).astype('uint8')  # Apply threshold to create binary localization mask\n",
    "    \n",
    "    dmg_msk = y_pred[..., 1:].argmax(axis=2) + 1  # Get the damage class (argmax over the remaining channels)\n",
    "    dmg_msk = dmg_msk * loc_msk  # Mask the damage predictions using the localization mask (only where loc_msk == 1)\n",
    "    \n",
    "    loc_msk = loc_msk.astype('uint8')\n",
    "    dmg_msk = dmg_msk.astype('uint8')\n",
    "    # After generating the damage mask, check the unique values\n",
    "    print(\"Unique values in the damage mask:\", np.unique(dmg_msk))\n",
    "\n",
    "    \n",
    "    return loc_msk, dmg_msk\n",
    "\n",
    "def save_colored_predictions(loc_msk, dmg_msk, file_name):\n",
    "    # Convert localization and damage masks to color images\n",
    "    loc_color = mask_to_color(loc_msk, localization_color_map)\n",
    "    dmg_color = mask_to_color(dmg_msk, damage_color_map)\n",
    "    \n",
    "    # Save the colored images\n",
    "    loc_filename = file_name.replace('_pre_disaster', '_localization_disaster_prediction_colored')\n",
    "    dmg_filename = file_name.replace('_pre_disaster', '_damage_disaster_prediction_colored')\n",
    "    \n",
    "    cv2.imwrite(os.path.join(model_dir, loc_filename), loc_color)\n",
    "    cv2.imwrite(os.path.join(model_dir, dmg_filename), dmg_color)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_step(model, test_loader, device='cuda', threshold=0.38):\n",
    "    \"\"\"\n",
    "    Predicts and saves output masks (localization and damage) for all batches in the DataLoader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained model to use for prediction.\n",
    "        test_loader (torch.utils.data.DataLoader): Dataloader for the test dataset.\n",
    "        device (str): The device to run the model on ('cuda' or 'cpu').\n",
    "        threshold (float): Threshold for localization mask.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    model.to(device)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(test_loader):  # Loop over batches from the test_loader\n",
    "        x, fns = batch[\"img\"].float().to(device), batch[\"fn\"]  # Get the input images and filenames\n",
    "        \n",
    "        y_hat = model(x).float()  # Forward pass through the model\n",
    "        \n",
    "        for pred, fn in zip(y_hat.unbind(dim=0), fns):\n",
    "            file_name = fn.split('/')[-1]  # Extract file name from full path\n",
    "            \n",
    "            # Process the prediction (apply sigmoid, threshold, and generate masks)\n",
    "            loc_msk, msk_dmg = pred_one_image(pred, threshold=threshold)\n",
    "            \n",
    "            # Save the colored localization and damage masks\n",
    "            save_colored_predictions(loc_msk, msk_dmg, file_name)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predicting an image and generating masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test data loader\n",
    "images_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\Guatemala\"\n",
    "masks_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\Guatemala_masks\"\n",
    "processed_data = process_images(images_folder, masks_folder)\n",
    "dataloader = create_dataset(processed_data)\n",
    "# Assuming `model` is your trained model and `test_loader` is your test DataLoader\n",
    "test_results = predict_step(model, dataloader, device='cuda')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysing competition split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test data loader\n",
    "images_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\test\\\\images\"\n",
    "masks_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\test\\\\masks\"\n",
    "processed_data = process_images(images_folder, masks_folder)\n",
    "dataloader = create_dataset(processed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# After loading processed_data\n",
    "for i in range(3):  # Visualize first 3 samples\n",
    "    sample = processed_data[i]\n",
    "    img = sample['img'].numpy().transpose(1, 2, 0)  # Shape: (H, W, 6)\n",
    "    msk = sample['msk'].numpy().transpose(1, 2, 0)  # Shape: (H, W, C)\n",
    "\n",
    "    # Separate pre- and post-disaster images\n",
    "    pre_img = img[:, :, :3]   # First 3 channels\n",
    "    post_img = img[:, :, 3:]  # Last 3 channels\n",
    "\n",
    "    # De-normalize images for display\n",
    "    pre_img_display = (pre_img + 1) * 127.5 / 255\n",
    "    post_img_display = (post_img + 1) * 127.5 / 255\n",
    "\n",
    "    # Display the images and masks\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Display pre-disaster image\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.imshow(pre_img_display)\n",
    "    plt.title('Pre-Disaster Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Display post-disaster image\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.imshow(post_img_display)\n",
    "    plt.title('Post-Disaster Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Display the first mask channel (e.g., building presence)\n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.imshow(msk[..., 0], cmap='gray')\n",
    "    plt.title('Mask Channel 0')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Display the second mask channel (e.g., damage level 1)\n",
    "    plt.subplot(1, 4, 4)\n",
    "    plt.imshow(msk[..., 1], cmap='gray')\n",
    "    plt.title('Mask Channel 1')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `model` is your trained model and `test_loader` is your test DataLoader\n",
    "test_results = evaluate_model(model, dataloader, device='cuda')\n",
    "\n",
    "# Print the F1 scores for each channel\n",
    "for channel, f1_score in test_results.items():\n",
    "    print(f\"{channel}: {f1_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis of arkansas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test data loader\n",
    "images_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\Arkansas\"\n",
    "masks_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\Arkansas_masks\"\n",
    "processed_data = process_images(images_folder, masks_folder)\n",
    "dataloader = create_dataset(processed_data)\n",
    "# Assuming `model` is your trained model and `test_loader` is your test DataLoader\n",
    "california_results = evaluate_model(model, dataloader, device='cuda')\n",
    "\n",
    "# Print the F1 scores for each channel\n",
    "for channel, f1_score in california_results.items():\n",
    "    print(f\"{channel}: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis of california"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test data loader\n",
    "images_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\California\"\n",
    "masks_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\California_masks\"\n",
    "processed_data = process_images(images_folder, masks_folder)\n",
    "dataloader = create_dataset(processed_data)\n",
    "# Assuming `model` is your trained model and `test_loader` is your test DataLoader\n",
    "california_results = evaluate_model(model, dataloader, device='cuda')\n",
    "\n",
    "# Print the F1 scores for each channel\n",
    "for channel, f1_score in california_results.items():\n",
    "    print(f\"{channel}: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis of mexico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test data loader\n",
    "images_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\Mexico\"\n",
    "masks_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\Mexico_masks\"\n",
    "processed_data = process_images(images_folder, masks_folder)\n",
    "dataloader = create_dataset(processed_data)\n",
    "# Assuming `model` is your trained model and `test_loader` is your test DataLoader\n",
    "mexico_results = evaluate_model(model, dataloader, device='cuda')\n",
    "\n",
    "# Print the F1 scores for each channel\n",
    "for channel, f1_score in mexico_results.items():\n",
    "    print(f\"{channel}: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis of florida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test data loader\n",
    "images_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\Florida\"\n",
    "masks_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\Florida_masks\"\n",
    "processed_data = process_images(images_folder, masks_folder)\n",
    "dataloader = create_dataset(processed_data)\n",
    "# Assuming `model` is your trained model and `test_loader` is your test DataLoader\n",
    "florida_results = evaluate_model(model, dataloader, device='cuda')\n",
    "\n",
    "# Print the F1 scores for each channel\n",
    "for channel, f1_score in florida_results.items():\n",
    "    print(f\"{channel}: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis of hawai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test data loader\n",
    "images_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\Hawaii\"\n",
    "masks_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\Hawaii_masks\"\n",
    "processed_data = process_images(images_folder, masks_folder)\n",
    "dataloader = create_dataset(processed_data)\n",
    "# Assuming `model` is your trained model and `test_loader` is your test DataLoader\n",
    "texas_results = evaluate_model(model, dataloader, device='cuda')\n",
    "\n",
    "# Print the F1 scores for each channel\n",
    "for channel, f1_score in texas_results.items():\n",
    "    print(f\"{channel}: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis of Missouri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test data loader\n",
    "images_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\Missouri\"\n",
    "masks_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\Missouri_masks\"\n",
    "processed_data = process_images(images_folder, masks_folder)\n",
    "dataloader = create_dataset(processed_data)\n",
    "# Assuming `model` is your trained model and `test_loader` is your test DataLoader\n",
    "texas_results = evaluate_model(model, dataloader, device='cuda')\n",
    "\n",
    "# Print the F1 scores for each channel\n",
    "for channel, f1_score in texas_results.items():\n",
    "    print(f\"{channel}: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis of Nepal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test data loader\n",
    "images_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\Nepal\"\n",
    "masks_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\Nepal_masks\"\n",
    "processed_data = process_images(images_folder, masks_folder)\n",
    "dataloader = create_dataset(processed_data)\n",
    "# Assuming `model` is your trained model and `test_loader` is your test DataLoader\n",
    "texas_results = evaluate_model(model, dataloader, device='cuda')\n",
    "\n",
    "# Print the F1 scores for each channel\n",
    "for channel, f1_score in texas_results.items():\n",
    "    print(f\"{channel}: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis of oklahoma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test data loader\n",
    "images_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\oklahoma\"\n",
    "masks_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\oklahoma_masks\"\n",
    "processed_data = process_images(images_folder, masks_folder)\n",
    "dataloader = create_dataset(processed_data)\n",
    "# Assuming `model` is your trained model and `test_loader` is your test DataLoader\n",
    "texas_results = evaluate_model(model, dataloader, device='cuda')\n",
    "\n",
    "# Print the F1 scores for each channel\n",
    "for channel, f1_score in texas_results.items():\n",
    "    print(f\"{channel}: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis of portugal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test data loader\n",
    "images_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\Portugal\"\n",
    "masks_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\Portugal_masks\"\n",
    "processed_data = process_images(images_folder, masks_folder)\n",
    "dataloader = create_dataset(processed_data)\n",
    "# Assuming `model` is your trained model and `test_loader` is your test DataLoader\n",
    "texas_results = evaluate_model(model, dataloader, device='cuda')\n",
    "\n",
    "# Print the F1 scores for each channel\n",
    "for channel, f1_score in texas_results.items():\n",
    "    print(f\"{channel}: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis of south australia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test data loader\n",
    "images_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\South_australia\"\n",
    "masks_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\South_australia_masks\"\n",
    "processed_data = process_images(images_folder, masks_folder)\n",
    "dataloader = create_dataset(processed_data)\n",
    "# Assuming `model` is your trained model and `test_loader` is your test DataLoader\n",
    "texas_results = evaluate_model(model, dataloader, device='cuda')\n",
    "\n",
    "# Print the F1 scores for each channel\n",
    "for channel, f1_score in texas_results.items():\n",
    "    print(f\"{channel}: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis of south carolina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test data loader\n",
    "images_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\South_Carolina\"\n",
    "masks_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\South_Carolina_masks\"\n",
    "processed_data = process_images(images_folder, masks_folder)\n",
    "dataloader = create_dataset(processed_data)\n",
    "# Assuming `model` is your trained model and `test_loader` is your test DataLoader\n",
    "carolina_results = evaluate_model(model, dataloader, device='cuda')\n",
    "\n",
    "# Print the F1 scores for each channel\n",
    "for channel, f1_score in carolina_results.items():\n",
    "    print(f\"{channel}: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis of indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test data loader\n",
    "images_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\Indonesia\"\n",
    "masks_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\Indonesia_masks\"\n",
    "processed_data = process_images(images_folder, masks_folder)\n",
    "dataloader = create_dataset(processed_data)\n",
    "# Assuming `model` is your trained model and `test_loader` is your test DataLoader\n",
    "indonesia_results = evaluate_model(model, dataloader, device='cuda')\n",
    "\n",
    "# Print the F1 scores for each channel\n",
    "for channel, f1_score in indonesia_results.items():\n",
    "    print(f\"{channel}: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis of texas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test data loader\n",
    "images_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\Texas\"\n",
    "masks_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\Texas_masks\"\n",
    "processed_data = process_images(images_folder, masks_folder)\n",
    "dataloader = create_dataset(processed_data)\n",
    "# Assuming `model` is your trained model and `test_loader` is your test DataLoader\n",
    "texas_results = evaluate_model(model, dataloader, device='cuda')\n",
    "\n",
    "# Print the F1 scores for each channel\n",
    "for channel, f1_score in texas_results.items():\n",
    "    print(f\"{channel}: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis of locations not seen by the model\n",
    "\n",
    "# 1. alabama\n",
    "# 2. Guatemala\n",
    "# 3. Sunda\n",
    "# 4. ayiti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of alabama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test data loader\n",
    "images_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\Alabama\"\n",
    "masks_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\Alabama_masks\"\n",
    "processed_data = process_images(images_folder, masks_folder)\n",
    "dataloader = create_dataset(processed_data)\n",
    "# Assuming `model` is your trained model and `test_loader` is your test DataLoader\n",
    "texas_results = evaluate_model(model, dataloader, device='cuda')\n",
    "\n",
    "# Print the F1 scores for each channel\n",
    "for channel, f1_score in texas_results.items():\n",
    "    print(f\"{channel}: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis of Guatemala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test data loader\n",
    "images_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\Guatemala\"\n",
    "masks_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\Guatemala_masks\"\n",
    "processed_data = process_images(images_folder, masks_folder)\n",
    "dataloader = create_dataset(processed_data)\n",
    "# Assuming `model` is your trained model and `test_loader` is your test DataLoader\n",
    "texas_results = evaluate_model(model, dataloader, device='cuda')\n",
    "\n",
    "# Print the F1 scores for each channel\n",
    "for channel, f1_score in texas_results.items():\n",
    "    print(f\"{channel}: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis if sunda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test data loader\n",
    "images_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\sunda\"\n",
    "masks_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\sunda_masks\"\n",
    "processed_data = process_images(images_folder, masks_folder)\n",
    "dataloader = create_dataset(processed_data)\n",
    "# Assuming `model` is your trained model and `test_loader` is your test DataLoader\n",
    "texas_results = evaluate_model(model, dataloader, device='cuda')\n",
    "\n",
    "# Print the F1 scores for each channel\n",
    "for channel, f1_score in texas_results.items():\n",
    "    print(f\"{channel}: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis of Ayiti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test data loader\n",
    "images_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\Ayiti\"\n",
    "masks_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\location_test_data\\\\Ayiti_masks\"\n",
    "processed_data = process_images(images_folder, masks_folder)\n",
    "dataloader = create_dataset(processed_data)\n",
    "# Assuming `model` is your trained model and `test_loader` is your test DataLoader\n",
    "texas_results = evaluate_model(model, dataloader, device='cuda')\n",
    "\n",
    "# Print the F1 scores for each channel\n",
    "for channel, f1_score in texas_results.items():\n",
    "    print(f\"{channel}: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysing out-of-domain dataset (Ida-BD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test data loader\n",
    "images_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\Ida-BD_dataset\\\\images\"\n",
    "masks_folder = \"C:\\\\Users\\\\PC\\\\Desktop\\\\damage_assessement_data\\\\Ida-BD_dataset\\\\masks\"\n",
    "processed_data = process_images(images_folder, masks_folder)\n",
    "dataloader = create_dataset(processed_data)\n",
    "# Assuming `model` is your trained model and `test_loader` is your test DataLoader\n",
    "ida_bd_results = evaluate_model(model, dataloader, device='cuda')\n",
    "\n",
    "# Print the F1 scores for each channel\n",
    "for channel, f1_score in ida_bd_results.items():\n",
    "    print(f\"{channel}: {f1_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
